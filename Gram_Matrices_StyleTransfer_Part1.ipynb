{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cf3a9c45",
   "metadata": {},
   "source": [
    "# Gram Matrices for Style (Neural Style Transfer) — Visual Demo\n",
    "\n",
    "This notebook builds the **first part** of the larger demo:\n",
    "**Load 4 images → run a fixed pretrained CNN → compute Gram matrices at selected layers → sanity-check everything.**\n",
    "\n",
    "Planned later (not yet in this part):\n",
    "- Pairwise similarity (cosine / L2) on style embeddings\n",
    "- PCA “downsampling” of Gram matrices for visualization\n",
    "- A distance heatmap across images\n",
    "\n",
    "---\n",
    "\n",
    "## What you provide\n",
    "Set the 4 URLs in the next cell:\n",
    "- 2 images from “Style A” (e.g., Van Gogh)\n",
    "- 2 images from “Style B” (e.g., animation / Disney-like)\n",
    "\n",
    "**Tip:** Keep images reasonably sized (≤ 2000px wide). We'll resize anyway.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "433519fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------\n",
    "# Required utility: load_image(url)\n",
    "from io import BytesIO\n",
    "import requests\n",
    "from PIL import Image\n",
    "\n",
    "def load_image(url):\n",
    "    '''Downloads and returns a PIL image from the given URL.'''\n",
    "    headers = {\"User-Agent\": \"Mozilla/5.0\"}\n",
    "    response = requests.get(url, headers=headers)\n",
    "    if response.status_code != 200:\n",
    "        raise ValueError(f\"Failed to download image: {url}\")\n",
    "    try:\n",
    "        return Image.open(BytesIO(response.content)).convert(\"RGB\")\n",
    "    except Exception as e:\n",
    "        raise ValueError(f\"Could not open image from {url}. Error: {e}\")\n",
    "# -------------------------\n",
    "\n",
    "# Sanity\n",
    "assert callable(load_image)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0c65258",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Provide your 4 image URLs here\n",
    "URLS = {\n",
    "    \"vangogh_1\": \"PASTE_URL_HERE\",\n",
    "    \"vangogh_2\": \"PASTE_URL_HERE\",\n",
    "    \"disney_1\":  \"PASTE_URL_HERE\",\n",
    "    \"disney_2\":  \"PASTE_URL_HERE\",\n",
    "}\n",
    "\n",
    "# Sanity\n",
    "assert isinstance(URLS, dict) and len(URLS) == 4\n",
    "for k, v in URLS.items():\n",
    "    assert isinstance(k, str) and isinstance(v, str)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b3f957e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as T\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from torchvision.models import vgg19, VGG19_Weights\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"device:\", device)\n",
    "\n",
    "# Sanity\n",
    "assert device in (\"cuda\", \"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "784ed4dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def assert_tensor(t: torch.Tensor, *, shape=None, finite=True, name=\"tensor\"):\n",
    "    assert isinstance(t, torch.Tensor), f\"{name} is not a torch.Tensor\"\n",
    "    if shape is not None:\n",
    "        assert tuple(t.shape) == tuple(shape), f\"{name}.shape={tuple(t.shape)} expected={tuple(shape)}\"\n",
    "    if finite:\n",
    "        assert torch.isfinite(t).all().item(), f\"{name} contains non-finite values\"\n",
    "\n",
    "def assert_symmetric_psd(G: torch.Tensor, *, tol_sym=1e-5, tol_eig=1e-6, name=\"G\"):\n",
    "    \"\"\"For a true Gram matrix: symmetric, PSD. Small negative eigenvalues may occur due to float error.\"\"\"\n",
    "    assert_tensor(G, name=name)\n",
    "    assert G.ndim == 2 and G.shape[0] == G.shape[1], f\"{name} must be square\"\n",
    "    sym_err = (G - G.T).abs().max().item()\n",
    "    assert sym_err < tol_sym, f\"{name} not symmetric enough: max|G-G^T|={sym_err}\"\n",
    "    Gcpu = G.detach().cpu()\n",
    "    eigvals = torch.linalg.eigvalsh(Gcpu)\n",
    "    min_eig = eigvals.min().item()\n",
    "    assert min_eig >= -tol_eig, f\"{name} not PSD within tolerance: min_eig={min_eig}\"\n",
    "    assert (torch.diag(Gcpu) >= -tol_eig).all().item(), f\"{name} has negative diagonal entries\"\n",
    "\n",
    "tmp = torch.eye(4)\n",
    "assert_symmetric_psd(tmp, name=\"I4\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "048bb935",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the 4 images\n",
    "imgs_pil = {k: load_image(u) for k, u in URLS.items()}\n",
    "\n",
    "# Show them\n",
    "plt.figure(figsize=(10, 6))\n",
    "for i, (k, im) in enumerate(imgs_pil.items(), start=1):\n",
    "    plt.subplot(2, 2, i)\n",
    "    plt.imshow(im)\n",
    "    plt.title(k)\n",
    "    plt.axis(\"off\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Sanity\n",
    "assert len(imgs_pil) == 4\n",
    "for k, im in imgs_pil.items():\n",
    "    assert hasattr(im, \"size\")\n",
    "    w, h = im.size\n",
    "    assert w > 10 and h > 10\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eec736df",
   "metadata": {},
   "source": [
    "## Fixed pretrained model\n",
    "\n",
    "We use a fixed pretrained **VGG-19** (features only).\n",
    "No training. No finetuning. We compute feature maps and Gram matrices.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99c903e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = VGG19_Weights.DEFAULT\n",
    "vgg = vgg19(weights=weights).features.to(device).eval()\n",
    "for p in vgg.parameters():\n",
    "    p.requires_grad_(False)\n",
    "\n",
    "IMG_SIZE = 256\n",
    "preprocess = T.Compose([\n",
    "    T.Resize(IMG_SIZE),\n",
    "    T.CenterCrop(IMG_SIZE),\n",
    "    T.ToTensor(),\n",
    "    T.Normalize(mean=weights.transforms().mean, std=weights.transforms().std),\n",
    "])\n",
    "\n",
    "# Sanity\n",
    "assert isinstance(vgg, nn.Module)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aabc798",
   "metadata": {},
   "source": [
    "## Choose layers for \"style\"\n",
    "\n",
    "We pick layer indices from `vgg.features` corresponding to ReLU outputs.\n",
    "We verify shapes rather than rely on layer-name assumptions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9d3a849",
   "metadata": {},
   "outputs": [],
   "source": [
    "LAYER_IDS = [1, 6, 11, 20]\n",
    "\n",
    "# Sanity\n",
    "assert all(isinstance(i, int) for i in LAYER_IDS)\n",
    "assert len(set(LAYER_IDS)) == len(LAYER_IDS)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a40cfc09",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pil_to_tensor(im_pil):\n",
    "    x = preprocess(im_pil).unsqueeze(0)  # (1,3,H,W)\n",
    "    return x.to(device)\n",
    "\n",
    "def extract_features(x: torch.Tensor, layer_ids=LAYER_IDS):\n",
    "    feats = {}\n",
    "    h = x\n",
    "    for i, layer in enumerate(vgg):\n",
    "        h = layer(h)\n",
    "        if i in layer_ids:\n",
    "            feats[i] = h\n",
    "    return feats\n",
    "\n",
    "features = {}\n",
    "for name, im in imgs_pil.items():\n",
    "    x = pil_to_tensor(im)\n",
    "    feats = extract_features(x)\n",
    "    features[name] = feats\n",
    "\n",
    "# Sanity\n",
    "for name in features:\n",
    "    feats = features[name]\n",
    "    assert set(feats.keys()) == set(LAYER_IDS), f\"{name} missing layers\"\n",
    "    for lid, act in feats.items():\n",
    "        assert act.ndim == 4 and act.shape[0] == 1, f\"{name} layer {lid} bad shape {tuple(act.shape)}\"\n",
    "        assert_tensor(act, name=f\"act[{name}][{lid}]\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bcf4f48",
   "metadata": {},
   "source": [
    "## Gram matrix\n",
    "\n",
    "For a feature map $A \\in \\mathbb{R}^{1 \\times C \\times H \\times W}$:\n",
    "\n",
    "1. Flatten spatial dims: $F \\in \\mathbb{R}^{C \\times N}$, where $N = HW$\n",
    "2. Gram: $G = FF^T \\in \\mathbb{R}^{C \\times C}$\n",
    "\n",
    "We normalize by **(C·H·W)** so layers don't dominate only due to size.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9232c7e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gram_matrix(act: torch.Tensor, normalize=True) -> torch.Tensor:\n",
    "    assert act.ndim == 4 and act.shape[0] == 1\n",
    "    _, C, H, W = act.shape\n",
    "    F = act[0].reshape(C, H * W)   # (C, N)\n",
    "    G = F @ F.t()                 # (C, C)\n",
    "    if normalize:\n",
    "        G = G / (C * H * W)\n",
    "    return G\n",
    "\n",
    "grams = {}\n",
    "for name in features:\n",
    "    grams[name] = {}\n",
    "    for lid, act in features[name].items():\n",
    "        G = gram_matrix(act, normalize=True).detach().cpu()\n",
    "        grams[name][lid] = G\n",
    "\n",
    "# Sanity\n",
    "for name in grams:\n",
    "    for lid, G in grams[name].items():\n",
    "        assert_symmetric_psd(G, name=f\"G[{name}][{lid}]\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31637c78",
   "metadata": {},
   "source": [
    "## Visual sanity check (small layer)\n",
    "\n",
    "Gram matrices get huge in deeper layers.\n",
    "We visualize an early layer where the heatmap is readable.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "322021e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "viz_lid = LAYER_IDS[0]\n",
    "C = next(iter(grams.values()))[viz_lid].shape[0]\n",
    "print(\"Visualizing layer\", viz_lid, \"with C =\", C)\n",
    "assert C <= 128, \"Pick an early layer with <=128 channels for readability.\"\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "for i, (name, d) in enumerate(grams.items(), start=1):\n",
    "    G = d[viz_lid].numpy()\n",
    "    plt.subplot(2, 2, i)\n",
    "    plt.imshow(G)\n",
    "    plt.title(f\"{name} — Gram @ layer {viz_lid} (C={G.shape[0]})\")\n",
    "    plt.axis(\"off\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d487489",
   "metadata": {},
   "source": [
    "## Note (for later): explicit layer weights\n",
    "\n",
    "Two separate choices:\n",
    "\n",
    "1. **Gram normalization** by size (we already do this): divides by $C \\cdot H \\cdot W$.\n",
    "   This is about fair scaling and stable gradients.\n",
    "\n",
    "2. **Layer weights** $w_\\ell$ in the final style loss:\n",
    "$\\mathcal{L}_{style} = \\sum_\\ell w_\\ell \\|G_\\ell^{gen} - G_\\ell^{style}\\|^2$\n",
    "\n",
    "Where do $w_\\ell$ come from?\n",
    "- Many references use **uniform** weights (all 1’s).\n",
    "- Others hand-tune weights to emphasize fine texture (early layers) vs larger motifs (mid layers).\n",
    "- For teaching: treat weights as a **preference knob**, not a constant derived from theory.\n",
    "\n",
    "We'll keep it simple later (uniform weights), and optionally add an interactive slider to explore the effect.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bddb65c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# End-of-part checkpoint: what we have so far\n",
    "\n",
    "print(\"Images:\", list(imgs_pil.keys()))\n",
    "print(\"Layers:\", LAYER_IDS)\n",
    "for name in grams:\n",
    "    for lid in LAYER_IDS:\n",
    "        G = grams[name][lid]\n",
    "        print(f\"{name:10s} layer {lid:2d} Gram shape:\", tuple(G.shape))\n",
    "\n",
    "# Sanity: all PSD\n",
    "assert len(grams) == 4\n",
    "for name in grams:\n",
    "    assert set(grams[name].keys()) == set(LAYER_IDS)\n",
    "    for lid in LAYER_IDS:\n",
    "        assert_symmetric_psd(grams[name][lid], name=f\"G[{name}][{lid}]\")\n",
    "\n",
    "print(\"\\n✅ Part 1 complete: Gram matrices computed and sanity-checked.\")\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}